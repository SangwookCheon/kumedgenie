import streamlit as st
import os
import glob
import warnings
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredMarkdownLoader,
    UnstructuredWordDocumentLoader,
    TextLoader
)
from langchain_core.prompts import PromptTemplate
from langchain_community.callbacks import get_openai_callback

# Load environment variables
load_dotenv()

# HYPERPARAMETERS
n_search_kwargs = 3

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)

# Set API Key
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("âŒ OPENAI_API_KEY is missing. Please set it in `.env` or Streamlit Secrets.")
else:
    os.environ["OPENAI_API_KEY"] = openai_api_key

# Initialize session state for retrieved chunks
if "retrieved_chunks" not in st.session_state:
    st.session_state["retrieved_chunks"] = []

# Streamlit UI
st.title("ğŸ¯ğŸ”ğŸ§â€â™€ï¸ ê³ ë ¤ëŒ€ ì˜ê³¼ëŒ€í•™ ì •ë³´ ì§€ë‹ˆ")

# Define document storage folder
DOC_FOLDER = "temp_rec"
FAISS_INDEX_PATH = "faiss_index"

# Define supported file loaders
FILE_LOADERS = {
    "pdf": PyPDFLoader,
    "md": UnstructuredMarkdownLoader,
    "txt": TextLoader,
    "docx": UnstructuredWordDocumentLoader,
}

# Check if FAISS index exists
if os.path.exists(FAISS_INDEX_PATH):
    st.success("âœ… ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. ë°”ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”!")
    vectorstore = FAISS.load_local(FAISS_INDEX_PATH, OpenAIEmbeddings(), allow_dangerous_deserialization=True)
    retriever = vectorstore.as_retriever(search_kwargs={"k": n_search_kwargs})
else:
    st.warning("ğŸ“‚ ë¬¸ì„œë¥¼ ì²˜ìŒ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.")

    all_files = []
    for ext in FILE_LOADERS.keys():
        all_files.extend(glob.glob(os.path.join(DOC_FOLDER, f"*.{ext}")))

    if not all_files:
        st.error("âŒ ì§€ì›ë˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. `resources` í´ë”ì— íŒŒì¼ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")
    else:
        all_documents = []
        for file_path in all_files:
            ext = file_path.split(".")[-1].lower()
            if ext in FILE_LOADERS:
                loader = FILE_LOADERS[ext](file_path)
                documents = loader.load()

                # Store file source metadata for each document
                for doc in documents:
                    doc.metadata["source"] = os.path.basename(file_path)

                all_documents.extend(documents)

        # Split documents
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,
                                                       chunk_overlap=200,
                                                       separators=["\n# ", "\n## ", "\n### "])
        splits = text_splitter.split_documents(all_documents)

        # Embed and store in FAISS
        vectorstore = FAISS.from_documents(splits, OpenAIEmbeddings())
        vectorstore.save_local(FAISS_INDEX_PATH)  
        retriever = vectorstore.as_retriever(search_kwargs={"k": n_search_kwargs})

        st.success("âœ… ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! ì§ˆë¬¸í•˜ì„¸ìš”.")

# Custom RAG prompt
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "ì§ˆë¬¸ì„ í•  í•™ìƒì€ ê³ ë ¤ëŒ€ ì˜ëŒ€ìƒì´ê³ , ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì •ì¤‘í•˜ê³  ê³µì‹ì ì¸ í†¤ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”. "
        """ê°œì¸ì ì¸ ì˜ê²¬ì´ë‚˜ ì‚¬ì‹¤ì„ ìœ ì¶”í•˜ëŠ” ë“± ì¶”ì¸¡í•˜ì§€ ë§ê³ , ê°€ëŠ¥í•œ ê²½ìš°, ì¶œì²˜ì—ì„œ ì§ì ‘ ì¸ìš©(\" \")í•˜ê³ , ê´€ë ¨ ì¡°í•­ì´ ìˆìœ¼ë©´ ìƒëµí•˜ì§€ ë§ê³  ë‚˜ì—´í•˜ì„¸ìš”.
        ë˜í•œ, í•™ì , ê¸°ê°„, ë¹„ìš©, ì‹œê°„, ìˆ˜ì—… ë²ˆí˜¸, ì›¹í˜ì´ì§€ ë§í¬, ì¡°í•­ ë“± ìˆ˜ì¹˜ì  ë˜ëŠ” êµ¬ì²´ì ì¸ ì •ë³´ê°€ ìˆìœ¼ë©´ ìƒëµí•˜ì§€ ë§ê³  ì œê³µí•˜ì„¸ìš”. 
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì£¼ì–´ì§„ ì •ë³´ë¥¼ ì°¸ê³ í•´ë„ ì§€ë‚˜ì¹˜ê²Œ ë¶€ì •í™•í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ "ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”!"ë¼ê³  ë§í•˜ì„¸ìš”. 
        ë§ˆì§€ë§‰ì—ëŠ” "ì•„ë˜ ì¶œì²˜ë¥¼ í™•ì¸í•˜ì—¬ ë” ì •í™•í•œ ì •ë³´ë¥¼ ì–»ê±°ë‚˜ í•™ìƒíšŒ, ë˜ëŠ” ì˜ê³¼ëŒ€í•™ í–‰ì •ì‹¤ (02-2286-1125)ë¡œ ë¬¸ì˜í•˜ì„¸ìš”."ë¼ëŠ” ë¬¸êµ¬ë¥¼ ì ìœ¼ì„¸ìš”. \n\n"""
        "**í•«ì•µì˜ ì§ˆë¬¸:** {question}\n"
        "**ê´€ë ¨ ì •ë³´:**\n{context}\n\n"
        "**ë‹µë³€:**"
    )
)

# Set up LLM
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# Define function to retrieve and format context
def retrieve_and_format_docs(query):
    # with get_openai_callback() as cb:
    #     retrieved_docs = retriever.invoke(query)
    #     st.write(cb)

    retrieved_docs = retriever.invoke(query)
    retrieved_chunks_display = []

    for doc in retrieved_docs:
        source = doc.metadata.get("source", "Unknown Source")
        text = f"{doc.page_content}\n\nğŸ“Œ ì¶œì²˜: {source}"
        retrieved_chunks_display.append(text)

    # Store retrieved chunks in session state
    st.session_state["retrieved_chunks"] = retrieved_chunks_display  

    return "\n\n---\n\n".join(retrieved_chunks_display)  # Separate retrieved chunks

# Define RAG chain
rag_chain = (
    {"context": RunnablePassthrough(), "question": RunnablePassthrough()}
    | custom_prompt
    | llm
    | StrOutputParser()
)

# User input field
with st.form(key="question_form"):
    query = st.text_input("ì§ˆë¬¸í•˜ì„¸ìš”:")
    submit_button = st.form_submit_button("Submit")

if submit_button:
    if query:
        # Retrieve context BEFORE invoking RAG
        formatted_context = retrieve_and_format_docs(query)

        # Invoke RAG with retrieved context
        response = rag_chain.invoke({"context": formatted_context, "question": query})

        # Show answer
        st.write("### ì§€ë‹ˆì˜ ë‹µë³€:")
        st.write(response)

        # **Add an expandable section for the retrieved chunks**
        with st.expander("ğŸ“Œ ì¶œì²˜ í‘œì‹œ"):
            if "retrieved_chunks" in st.session_state and st.session_state["retrieved_chunks"]:
                st.write("### ğŸ“Œ ì°¸ê³ í•œ ë¬¸ì„œ ë‚´ìš©:")
                for chunk in st.session_state["retrieved_chunks"]:
                    st.write(chunk)  # Show first 500 characters for readability
                    st.write("\n")
            else:
                st.write("âŒ ì°¸ê³ í•œ ë¬¸ì„œ ì¡°ê°ì´ ì—†ìŠµë‹ˆë‹¤.")

    else:
        st.warning("ì§ˆë¬¸ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.")