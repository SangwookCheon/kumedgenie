import streamlit as st
import os
import glob
import warnings
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredMarkdownLoader, # NOTE: This is throwing me NLTK errors
    UnstructuredWordDocumentLoader,
    TextLoader
)
from langchain_core.prompts import PromptTemplate
load_dotenv()
# Suppress warnings (e.g., PyPDFLoader warnings)
warnings.filterwarnings("ignore", category=UserWarning)

# Set API Key
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

# Streamlit UI
st.title("ğŸ” ê³ ë ¤ëŒ€ ì˜ê³¼ëŒ€í•™ ì •ë³´ ì§€ë‹ˆ")

# Define document storage folder and database path
DOC_FOLDER = "resources"
CHROMA_DB_PATH = "./chroma_db"

# Define supported file loaders
FILE_LOADERS = {
    "pdf": PyPDFLoader,
    "md": UnstructuredMarkdownLoader,
    "txt": TextLoader,
    "docx": UnstructuredWordDocumentLoader,
}

# Check if ChromaDB already exists
if os.path.exists(CHROMA_DB_PATH):
    st.success("âœ… ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. ë°”ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”!")
    vectorstore = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=OpenAIEmbeddings())
    retriever = vectorstore.as_retriever()
else:
    # If no database exists, process documents
    st.warning("ğŸ“‚ ë¬¸ì„œë¥¼ ì²˜ìŒ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.")
    
    all_files = []
    for ext in FILE_LOADERS.keys():
        all_files.extend(glob.glob(os.path.join(DOC_FOLDER, f"*.{ext}")))

    if not all_files:
        st.error("âŒ ì§€ì›ë˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. `resources` í´ë”ì— íŒŒì¼ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")
    else:
        all_documents = []
        for file_path in all_files:
            ext = file_path.split(".")[-1].lower()
            if ext in FILE_LOADERS:
                loader = FILE_LOADERS[ext](file_path)
                documents = loader.load()
                all_documents.extend(documents)

        # Split documents
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(all_documents)

        # Embed and store in ChromaDB
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=OpenAIEmbeddings(),
            persist_directory=CHROMA_DB_PATH
        )
        retriever = vectorstore.as_retriever()
        st.success("âœ… ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! ì§ˆë¬¸í•˜ì„¸ìš”.")

# Custom RAG prompt
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "ì§ˆë¬¸ì„ í•  í•™ìƒì€ ê³ ë ¤ëŒ€ ì˜ëŒ€ìƒì´ê³ , ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. "
        "ëª¨ë“  ë‹µë³€ì€ **í•œêµ­ì–´**ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•˜ë©°, ì •ì¤‘í•˜ê³  ê³µì‹ì ì¸ í†¤ì„ ìœ ì§€í•˜ê³ , ê°œì¸ì ì¸ ì˜ê²¬ì´ ì—†ì–´ì•¼ ë©ë‹ˆë‹¤. ì¶œì²˜ì—ì„œ ì œê³µë˜ëŠ” ì‚¬ì‹¤ë§Œ ë§í•˜ê³  í™•ëŒ€ í•´ì„í•˜ì§€ ë§ˆì„¸ìš”."
        """ê°€ëŠ¥í•œ ê²½ìš°, ì¶œì²˜ì—ì„œ ì§ì ‘ ì¸ìš©(\" \")í•˜ê³ , ê´€ë ¨ ì¡°í•­ì´ ìˆìœ¼ë©´ ìƒëµí•˜ì§€ ë§ê³  ë‚˜ì—´í•˜ì„¸ìš”.
        ë˜í•œ, í•™ì , ê¸°ê°„, ë¹„ìš©, ì‹œê°„, ìˆ˜ì—… ë²ˆí˜¸, ì›¹í˜ì´ì§€ ë§í¬, ì¡°í•­ ë“± ìˆ˜ì¹˜ì  ë˜ëŠ” êµ¬ì²´ì ì¸ ì •ë³´ê°€ ìˆìœ¼ë©´ ìƒëµí•˜ì§€ ë§ê³  ì œê³µí•˜ì„¸ìš”. 
        test.txtì—ëŠ” ê´€ë ¨ ì›¹ì‚¬ì´íŠ¸ ë§í¬ê°€ ë‚˜ì—´ë˜ì–´ ìˆëŠ”ë° ì§ˆë¬¸ì— ê´€ë ¨ëœ ì›¹ì‚¬ì´íŠ¸ ë§í¬ë¥¼ ë§ˆì§€ë§‰ ì¤„ì— "ê´€ë ¨ ì›¹í˜ì´ì§€: "ì“°ê³  ë§í¬ ì‘ì„±í•˜ì„¸ìš”. 
        ì§ˆë¬¸ì´ ì§€ë‚˜ì¹˜ê²Œ ì •í™•í•˜ì§€ ì•Šì€ ê²ƒ ê°™ìœ¼ë©´ ì§ˆë¬¸ì´ ì •í™•í•œì§€ í™•ì¸í•˜ê±°ë‚˜ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ë‹¬ë¼ê³  ë¶€íƒí•˜ì„¸ìš”. 
        ì¶œì²˜ì—ì„œ ì •ë³´ê°€ ì •í™•í•˜ì§€ ì•Šìœ¼ë©´ ê´€ë ¨ ì •ë³´ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ë¼ê³  í•˜ì„¸ìš”. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë§ˆì§€ë§‰ì— 'ì´ ì •ë³´ëŠ” ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'ë¼ëŠ” ê²½ê³  ë¬¸êµ¬ë¥¼ í¬í•¨í•˜ì„¸ìš”. \n\n"""
        "**ì§ˆë¬¸:** {question}\n"
        "**ì¶œì²˜:**\n{context}\n\n"
        "**ë‹µë³€:**"
    )
)

# Set up LLM
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# Define RAG chain
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | custom_prompt
    | llm
    | StrOutputParser()
)

# User input field
with st.form(key="question_form"):
    query = st.text_input("ì§ˆë¬¸í•˜ì„¸ìš”:")
    submit_button = st.form_submit_button("Submit")

if submit_button:
    if query:
        response = rag_chain.invoke(query)
        st.write("### ì§€ë‹ˆì˜ ë‹µë³€:")
        st.write(response)
    else:
        st.warning("ì§ˆë¬¸ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.")