import streamlit as st
import os
import glob
import warnings
import pandas as pd
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredMarkdownLoader,
    UnstructuredWordDocumentLoader,
    TextLoader
)
from langchain_core.prompts import PromptTemplate
from langchain_community.callbacks import get_openai_callback
import requests
from operator import itemgetter
import shutil
import time
from typing import Optional

# KUMEDGENIE 

#custom functions
from visit_counter import get_visit_count, update_visit_count, get_last_visit, update_last_visit  # Import visit tracking

# Load environment variables
load_dotenv()

# HYPERPARAMETERS
n_search_kwargs = 3

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)

# Set API Key
openai_api_key = os.getenv("OPENAI_API_KEY")
prompt_text = os.getenv("PROMPT_TEMPLATE")

if not openai_api_key:
    st.error("âŒ OPENAI_API_KEY is missing. Please set it in `.env` or Streamlit Secrets.")
else:
    os.environ["OPENAI_API_KEY"] = openai_api_key

# Initialize session state for retrieved chunks
if "retrieved_chunks" not in st.session_state:
    st.session_state["retrieved_chunks"] = []

# Visit Counter implementation
visit_count = update_visit_count()
last_visit = get_last_visit()
update_last_visit()

# Streamlit UI
st.title("ğŸ¯ğŸ” ê³ ë ¤ëŒ€ ì˜ê³¼ëŒ€í•™ ì •ë³´ ì§€ë‹ˆ")
st.write("ì§€ë‹ˆëŠ” ì‹ ì…ìƒ ìˆ˜ê°• ì‹ ì²­ ì •ë³´ì™€ ìœ ìš©í•œ íŒ, ê³ ë ¤ëŒ€í•™êµ ì˜ê³¼ëŒ€í•™ í•™ì¹™(íœ´Â·ë³µí•™, ì´ì¤‘ ì „ê³µ, ì¥í•™ìƒ ê¸°ì¤€ ë“±)ì— ëŒ€í•œ ê¶ê¸ˆì¦ì„ í•´ê²°í•´ ë“œë¦½ë‹ˆë‹¤!")

# Define document storage folder
DOC_FOLDER = "temp_rec"
FAISS_INDEX_PATH = "faiss_index"

# Define supported file loaders
FILE_LOADERS = {
    "pdf": PyPDFLoader,
    "md": UnstructuredMarkdownLoader,
    "txt": TextLoader,
    "docx": UnstructuredWordDocumentLoader,
}

# --------------------------
# GitHub Auto-pull Settings
# --------------------------
GITHUB_OWNER = "SangwookCheon"
GITHUB_REPO = "kumedgenie"
GITHUB_BRANCH = "main"
GITHUB_DOC_PATH = "temp_rec"  # folder in the repo that holds your docs
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")  # optional; required for private repos or high traffic

def _github_headers():
    h = {"Accept": "application/vnd.github+json"}
    if GITHUB_TOKEN:
        h["Authorization"] = f"Bearer {GITHUB_TOKEN}"
    return h

def _list_github_dir(owner, repo, path, branch="main"):
    """List items in a GitHub folder via Contents API."""
    api_url = f"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}"
    r = requests.get(api_url, headers=_github_headers(), timeout=30)
    r.raise_for_status()
    return r.json()

def _raw_github_url(owner, repo, branch, full_path):
    return f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{full_path}"

def _download_file(url, dest_path):
    os.makedirs(os.path.dirname(dest_path), exist_ok=True)
    with requests.get(url, headers=_github_headers(), timeout=60, stream=True) as r:
        r.raise_for_status()
        with open(dest_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

def _sync_dir_recursive(owner, repo, branch, repo_base_path, local_base_path):
    """
    Recursively mirrors the repo folder (repo_base_path) into local_base_path,
    but only downloads files whose extensions are supported by FILE_LOADERS.
    """
    items = _list_github_dir(owner, repo, repo_base_path, branch)
    allowed_exts = set(FILE_LOADERS.keys())
    downloaded_any = False

    for it in items:
        it_type = it.get("type")
        it_name = it.get("name")
        it_path = it.get("path")  # full path within repo

        if it_type == "dir":
            # Recurse into subdir
            sub_downloaded = _sync_dir_recursive(owner, repo, branch, it_path, local_base_path)
            downloaded_any = downloaded_any or sub_downloaded

        elif it_type == "file":
            ext = (it_name.split(".")[-1].lower() if "." in it_name else "")
            if ext in allowed_exts or it_name == "version.txt":
                raw_url = _raw_github_url(owner, repo, branch, it_path)
                relative_local = os.path.relpath(it_path, GITHUB_DOC_PATH)
                dest_path = os.path.join(local_base_path, relative_local) if ext in allowed_exts else os.path.join(FAISS_INDEX_PATH, "version.txt")
                # For content files (allowed_exts), place under DOC_FOLDER; for version.txt we keep using FAISS_INDEX_PATH/version.txt (handled elsewhere too)
                if ext in allowed_exts:
                    # place under DOC_FOLDER mirroring relative path
                    dest_path = os.path.join(local_base_path, relative_local)
                try:
                    _download_file(raw_url, dest_path)
                    downloaded_any = True
                except Exception as e:
                    st.error(f"âŒ ì›ê²© íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {it_name} ({e})")
    return downloaded_any

def sync_docs_from_github():
    """Clear DOC_FOLDER and repopulate with the latest files from GitHub (recursive)."""
    try:
        # Clean doc folder for a fresh mirror
        if os.path.exists(DOC_FOLDER):
            shutil.rmtree(DOC_FOLDER)
        os.makedirs(DOC_FOLDER, exist_ok=True)

        downloaded = _sync_dir_recursive(
            owner=GITHUB_OWNER,
            repo=GITHUB_REPO,
            branch=GITHUB_BRANCH,
            repo_base_path=GITHUB_DOC_PATH,
            local_base_path=DOC_FOLDER,
        )
        if not downloaded:
            st.warning("âš ï¸ ì›ê²© ì €ì¥ì†Œì—ì„œ ë‹¤ìš´ë¡œë“œí•  ì§€ì› íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return downloaded
    except requests.HTTPError as e:
        st.error(f"âŒ GitHub API ì˜¤ë¥˜: {e}")
        return False
    except Exception as e:
        st.error(f"âŒ ì›ê²© ë¬¸ì„œ ë™ê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return False

# --------------------------
# Versioning (remote/local)
# --------------------------

# Use GitHub raw for live; keep your local fake line commented for dev
REMOTE_VERSION_URL = "https://raw.githubusercontent.com/SangwookCheon/kumedgenie/main/temp_rec/version.txt"
# REMOTE_VERSION_URL = "file:///" + os.path.abspath("fake_remote_version.txt")

LOCAL_VERSION_PATH = os.path.join(FAISS_INDEX_PATH, "version.txt")

# ---- cached version fetch (prevents hammering GitHub) ----
@st.cache_data(ttl=60, show_spinner=False)
def cached_fetch_remote_version(url: str, headers: dict | None = None) -> Optional[str]:
    try:
        r = requests.get(url, headers=headers or {}, timeout=20)
        if r.status_code == 200:
            return r.text.strip()
    except Exception:
        return None
    return None

def get_remote_version():
    try:
        if REMOTE_VERSION_URL.startswith("file:///"):
            local_path = REMOTE_VERSION_URL.replace("file:///", "")
            with open(local_path, "r") as f:
                return f.read().strip()
        else:
            return cached_fetch_remote_version(REMOTE_VERSION_URL, _github_headers())
    except Exception as e:
        st.error(f"âŒ ì›ê²© ë²„ì „ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    return None

def get_local_version():
    if os.path.exists(LOCAL_VERSION_PATH):
        with open(LOCAL_VERSION_PATH, "r") as f:
            return f.read().strip()
    return None

def update_local_version(version):
    os.makedirs(FAISS_INDEX_PATH, exist_ok=True)
    with open(LOCAL_VERSION_PATH, "w") as f:
        f.write(version)

# --------------------------
# Build / Load Index Helpers
# --------------------------
def build_index_from_local_docs():
    all_files = []
    for ext in FILE_LOADERS.keys():
        all_files.extend(glob.glob(os.path.join(DOC_FOLDER, f"**/*.{ext}"), recursive=True))
        all_files.extend(glob.glob(os.path.join(DOC_FOLDER, f"*.{ext}")))  # in case no subfolders

    if not all_files:
        st.error("âŒ ì§€ì›ë˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
        return None, None

    all_documents = []
    for file_path in all_files:
        ext = file_path.split(".")[-1].lower()
        if ext in FILE_LOADERS:
            loader = FILE_LOADERS[ext](file_path)
            documents = loader.load()
            for doc in documents:
                # Keep only a relative, readable source name
                rel = os.path.relpath(file_path, DOC_FOLDER)
                doc.metadata["source"] = rel
            all_documents.extend(documents)

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)
    splits = text_splitter.split_documents(all_documents)

    vectorstore = FAISS.from_documents(splits, OpenAIEmbeddings())
    vectorstore.save_local(FAISS_INDEX_PATH)
    retriever_local = vectorstore.as_retriever(search_kwargs={"k": n_search_kwargs})
    return vectorstore, retriever_local

# --------------------------
# Rebuild lock (single-writer)
# --------------------------
LOCK_PATH = os.path.join(FAISS_INDEX_PATH, ".rebuild.lock")

class FileLock:
    def __init__(self, path, timeout=180):
        self.path = path
        self.timeout = timeout
    def __enter__(self):
        start = time.time()
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        while True:
            try:
                fd = os.open(self.path, os.O_CREAT | os.O_EXCL | os.O_WRONLY)
                os.close(fd)
                break
            except FileExistsError:
                if time.time() - start > self.timeout:
                    break
                time.sleep(0.2)
        return self
    def __exit__(self, exc_type, exc, tb):
        try:
            if os.path.exists(self.path):
                os.remove(self.path)
        except Exception:
            pass

# --------------------------
# Version check + Sync + Index
# --------------------------
remote_version = get_remote_version()
local_version = get_local_version()

needs_rebuild = (not os.path.exists(FAISS_INDEX_PATH)) or (remote_version and remote_version != local_version)

if needs_rebuild:
    st.warning("ğŸ“‚ ìƒˆë¡œìš´ ì›ê²© ë²„ì „ì„ ê°ì§€í–ˆìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¤ê³  ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í•©ë‹ˆë‹¤...")
    with FileLock(LOCK_PATH, timeout=180):
        # Re-check inside the lock in case another worker already rebuilt
        local_version = get_local_version()
        needs_rebuild_locked = (not os.path.exists(FAISS_INDEX_PATH)) or (remote_version and remote_version != local_version)

        if needs_rebuild_locked:
            # 1) Pull latest docs from GitHub into DOC_FOLDER
            synced = sync_docs_from_github()

            # 2) Build index from freshly synced docs
            if synced:
                vectorstore, retriever = build_index_from_local_docs()
                if vectorstore and retriever:
                    if remote_version:
                        update_local_version(remote_version)
                    st.success("âœ… ë¬¸ì„œë¥¼ ìµœì‹ ìœ¼ë¡œ ë™ê¸°í™”í•˜ê³  ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸í•˜ì„¸ìš”.")
                else:
                    st.error("âŒ ì¸ë±ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                # Even if sync failed, try to use existing local docs (best effort)
                vectorstore, retriever = build_index_from_local_docs()
                if vectorstore and retriever:
                    st.warning("âš ï¸ ì›ê²© ë™ê¸°í™”ì— ì‹¤íŒ¨í•˜ì—¬ ë¡œì»¬ ë¬¸ì„œë¡œ ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.error("âŒ ë¬¸ì„œ ë™ê¸°í™” ë° ì¸ë±ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        else:
            # Someone else finished rebuild while we waited â†’ just load existing
            vectorstore = FAISS.load_local(FAISS_INDEX_PATH, OpenAIEmbeddings(), allow_dangerous_deserialization=True)
            retriever = vectorstore.as_retriever(search_kwargs={"k": n_search_kwargs})
            st.success("âœ… ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. ì§ˆë¬¸í•˜ì„¸ìš”!")
else:
    # Load existing index
    vectorstore = FAISS.load_local(FAISS_INDEX_PATH, OpenAIEmbeddings(), allow_dangerous_deserialization=True)
    retriever = vectorstore.as_retriever(search_kwargs={"k": n_search_kwargs})
    st.success("âœ… ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. ì§ˆë¬¸í•˜ì„¸ìš”!")

# --------------------------
# Prompt + LLM
# --------------------------
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_text
)

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# --------------------------
# Retrieval + UI
# --------------------------
def retrieve_and_format_docs(query):
    retrieved_docs = retriever.invoke(query)
    retrieved_chunks_display = []

    for doc in retrieved_docs:
        source = doc.metadata.get("source", "Unknown Source")
        text = f"{doc.page_content}\n\nğŸ“Œ ì¶œì²˜: {source}"
        retrieved_chunks_display.append(text)

    # Store retrieved chunks in session state
    st.session_state["retrieved_chunks"] = retrieved_chunks_display
    return "\n\n---\n\n".join(retrieved_chunks_display)

# Define RAG chain
rag_chain = (
    {"context": itemgetter("context"), "question": itemgetter("question")}
    | custom_prompt
    | llm
    | StrOutputParser()
)

# User input field
with st.form(key="question_form"):
    query = st.text_input("ì§ˆë¬¸í•˜ì„¸ìš” (ì§ˆë¬¸ì´ êµ¬ì²´ì ì¼ìˆ˜ë¡ ë”ìš± ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤):")
    submit_button = st.form_submit_button("Submit")

if submit_button:
    if query:
        # Retrieve context BEFORE invoking RAG
        formatted_context = retrieve_and_format_docs(query)

        # Invoke RAG with retrieved context
        response = rag_chain.invoke({"context": formatted_context, "question": query})

        # Show answer
        st.write("### ì§€ë‹ˆì˜ ë‹µë³€:")
        st.write(response)

        # Add an expandable section for the retrieved chunks
        with st.expander("ğŸ“Œ ì¶œì²˜ í‘œì‹œ"):
            if "retrieved_chunks" in st.session_state and st.session_state["retrieved_chunks"]:
                st.write("### ğŸ“Œ ì°¸ê³ í•œ ë¬¸ì„œ ë‚´ìš©:")
                for chunk in st.session_state["retrieved_chunks"]:
                    st.write(chunk)
                    st.write("\n")
            else:
                st.write("âŒ ì°¸ê³ í•œ ë¬¸ì„œ ì¡°ê°ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("ì§ˆë¬¸ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.")

version_display = remote_version if remote_version else "ì•Œ ìˆ˜ ì—†ìŒ"

st.markdown(
f"""
<hr style="margin-top: 50px;">
<p style="text-align: center; font-size: 14px;">
    Made with â¤ï¸ by Wookie at &lt;/+/&gt;<br>
    <span style="font-size:12px; color:gray;">ë²„ì „: {version_display}</span>
</p>
""",
unsafe_allow_html=True
)

#    ë°©ë¬¸ íšŸìˆ˜: {visit_count}íšŒ | ë§ˆì§€ë§‰ ë°©ë¬¸: {last_visit} <br>