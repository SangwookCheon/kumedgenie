import streamlit as st
import os
import glob
import warnings
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredMarkdownLoader,  # NOTE: This may require NLTK
    UnstructuredWordDocumentLoader,
    TextLoader
)
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document

# Load environment variables
load_dotenv()

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)

# Set API Key
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("âŒ OPENAI_API_KEY is missing. Please set it in `.env` or Streamlit Secrets.")
else:
    os.environ["OPENAI_API_KEY"] = openai_api_key

# Streamlit UI
st.title("ğŸ” ê³ ë ¤ëŒ€ ì˜ê³¼ëŒ€í•™ ì •ë³´ ì§€ë‹ˆ")

# Define document storage folder
DOC_FOLDER = "temp_rec"
FAISS_INDEX_PATH = "faiss_index"

# Define supported file loaders
FILE_LOADERS = {
    "pdf": PyPDFLoader,
    "md": UnstructuredMarkdownLoader,
    "txt": TextLoader,
    "docx": UnstructuredWordDocumentLoader,
}

# Check if FAISS index exists
if os.path.exists(FAISS_INDEX_PATH):
    st.success("âœ… ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. ë°”ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”!")
    vectorstore = FAISS.load_local(FAISS_INDEX_PATH, OpenAIEmbeddings(), allow_dangerous_deserialization=True)
    retriever = vectorstore.as_retriever()
else:
    # If no FAISS index exists, process documents
    st.warning("ğŸ“‚ ë¬¸ì„œë¥¼ ì²˜ìŒ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.")

    all_files = []
    for ext in FILE_LOADERS.keys():
        all_files.extend(glob.glob(os.path.join(DOC_FOLDER, f"*.{ext}")))

    if not all_files:
        st.error("âŒ ì§€ì›ë˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. `resources` í´ë”ì— íŒŒì¼ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")
    else:
        all_documents = []
        for file_path in all_files:
            ext = file_path.split(".")[-1].lower()
            if ext in FILE_LOADERS:
                loader = FILE_LOADERS[ext](file_path)
                documents = loader.load()
                all_documents.extend(documents)

        # Split documents
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,
                                                       chunk_overlap=200,
                                                       separators=["\n# ", "\n## ", "\n### "])
        splits = text_splitter.split_documents(all_documents)

        for i, doc in enumerate(splits[:3]):
            print(f"\nChunk {i+1}:\n{doc.page_content}\n")

        # Embed and store in FAISS
        vectorstore = FAISS.from_documents(splits, OpenAIEmbeddings())
        vectorstore.save_local(FAISS_INDEX_PATH)  # Save index
        retriever = vectorstore.as_retriever() #search_kwargs={"k": 5}

        st.success("âœ… ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! ì§ˆë¬¸í•˜ì„¸ìš”.")

# Custom RAG prompt
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "ì§ˆë¬¸ì„ í•  í•™ìƒì€ ê³ ë ¤ëŒ€ ì˜ëŒ€ìƒì´ê³ , ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. "
        "ëª¨ë“  ë‹µë³€ì€ **í•œêµ­ì–´**ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•˜ë©°, ì •ì¤‘í•˜ê³  ê³µì‹ì ì¸ í†¤ì„ ìœ ì§€í•˜ê³ , ê°œì¸ì ì¸ ì˜ê²¬ì´ ì—†ì–´ì•¼ ë©ë‹ˆë‹¤. ì¶œì²˜ì—ì„œ ì œê³µë˜ëŠ” ì‚¬ì‹¤ë§Œ ë§í•˜ê³  í™•ëŒ€ í•´ì„í•˜ì§€ ë§ˆì„¸ìš”."
        """ê°€ëŠ¥í•œ ê²½ìš°, ì¶œì²˜ì—ì„œ ì§ì ‘ ì¸ìš©(\" \")í•˜ê³ , ê´€ë ¨ ì¡°í•­ì´ ìˆìœ¼ë©´ ìƒëµí•˜ì§€ ë§ê³  ë‚˜ì—´í•˜ì„¸ìš”.
        ë˜í•œ, í•™ì , ê¸°ê°„, ë¹„ìš©, ì‹œê°„, ìˆ˜ì—… ë²ˆí˜¸, ì›¹í˜ì´ì§€ ë§í¬, ì¡°í•­ ë“± ìˆ˜ì¹˜ì  ë˜ëŠ” êµ¬ì²´ì ì¸ ì •ë³´ê°€ ìˆìœ¼ë©´ ìƒëµí•˜ì§€ ë§ê³  ì œê³µí•˜ì„¸ìš”. 
        test.txtì—ëŠ” ê´€ë ¨ ì›¹ì‚¬ì´íŠ¸ ë§í¬ê°€ ë‚˜ì—´ë˜ì–´ ìˆëŠ”ë° ì§ˆë¬¸ì— ê´€ë ¨ëœ ì›¹ì‚¬ì´íŠ¸ ë§í¬ë¥¼ ë§ˆì§€ë§‰ ì¤„ì— "ê´€ë ¨ ì›¹í˜ì´ì§€: "ì“°ê³  ë§í¬ ì‘ì„±í•˜ì„¸ìš”. 
        ì§ˆë¬¸ì´ ì§€ë‚˜ì¹˜ê²Œ ì •í™•í•˜ì§€ ì•Šì€ ê²ƒ ê°™ìœ¼ë©´ ì§ˆë¬¸ì´ ì •í™•í•œì§€ í™•ì¸í•˜ê±°ë‚˜ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ë‹¬ë¼ê³  ë¶€íƒí•˜ì„¸ìš”. 
        ì¶œì²˜ì—ì„œ ì •ë³´ê°€ ì •í™•í•˜ì§€ ì•Šìœ¼ë©´ ê´€ë ¨ ì •ë³´ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ë¼ê³  í•˜ì„¸ìš”. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë§ˆì§€ë§‰ì— 'ì´ ì •ë³´ëŠ” ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'ë¼ëŠ” ê²½ê³  ë¬¸êµ¬ë¥¼ í¬í•¨í•˜ì„¸ìš”. \n\n"""
        "**ì§ˆë¬¸:** {question}\n"
        "**ì¶œì²˜:**\n{context}\n\n"
        "**ë‹µë³€:**"
    )
)

# Set up LLM
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# Define RAG chain
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | custom_prompt
    | llm
    | StrOutputParser()
)

# User input field
with st.form(key="question_form"):
    query = st.text_input("ì§ˆë¬¸í•˜ì„¸ìš”:")
    submit_button = st.form_submit_button("Submit")

if submit_button:
    if query:
        response = rag_chain.invoke(query)
        st.write("### ì§€ë‹ˆì˜ ë‹µë³€:")
        st.write(response)
    else:
        st.warning("ì§ˆë¬¸ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.")